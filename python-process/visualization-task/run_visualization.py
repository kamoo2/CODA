import argparse
import json
import base64
import os
import multiprocessing
import logging
import traceback
import time
from processor.execution.data_processor import DataProcessor
from processor.execution.file_processor_thread import FileProcessorThread
from processor.execution.segment_barrier import SegmentBarrier
from processor.util.s3_util import get_project_info_key, read_json_from_path
from processor.model.visualization_model import FileMetadata, Credential
from mqtt.mqtt_client_singleton import get_mqtt_client



# ë¡œê¹… ì„¤ì •
logging.basicConfig(
    level=logging.INFO,
    format='[%(asctime)s] [%(levelname)s] [%(name)s] %(message)s',
    datefmt='%H:%M:%S'
)
logger = logging.getLogger("RunVisualization")

def parse_time(ts: str):
    """ì‹œê°„ íŒŒì‹± - ì˜ˆì™¸ ì²˜ë¦¬ ê°•í™”"""
    import datetime
    try:
        return datetime.datetime.strptime(ts, "%Y:%m:%d:%H:%M:%S:%f")
    except ValueError as e:
        logger.error(f"âš ï¸ ì‹œê°„ íŒŒì‹± ì‹¤íŒ¨: '{ts}' - {e}")
        raise


def prepare_relative_offsets(json_path, files, credential):
    """íŒŒì¼ ìƒëŒ€ ì˜¤í”„ì…‹ ì¤€ë¹„ - ìµœì í™” + ì˜ˆì™¸ì²˜ë¦¬ ê°•í™”"""
    logger.info(f"ğŸ“‹ í”„ë¡œì íŠ¸ ì •ë³´ ë¡œë”© ì‹œì‘: {json_path}")
    start_time = time.time()

    try:
        data = read_json_from_path(json_path, credential.access_key, credential.secret_key,
                                   credential.region_name, credential.bucket_name)
        load_time = time.time() - start_time
        logger.info(f"âœ… í”„ë¡œì íŠ¸ ì •ë³´ ë¡œë”© ì™„ë£Œ: {load_time:.3f}ì´ˆ")

    except Exception as e:
        logger.error(f"âŒ í”„ë¡œì íŠ¸ ì •ë³´ ë¡œë”© ì‹¤íŒ¨: {e}")
        logger.error(f"ğŸ“„ JSON ê²½ë¡œ: {json_path}")
        logger.error(f"ğŸ”‘ í¬ë¦¬ë´ì…œ ì •ë³´: region={credential.region_name}, bucket={credential.bucket_name}")
        # ëª¨ë“  íŒŒì¼ì„ ì˜¤í”„ì…‹ 0ìœ¼ë¡œ ì²˜ë¦¬
        logger.warning("ğŸ”„ ëª¨ë“  íŒŒì¼ ì˜¤í”„ì…‹ì„ 0ìœ¼ë¡œ ì„¤ì •í•˜ì—¬ ê³„ì† ì§„í–‰")
        return {file.upload_file_path: 0 for file in files}

    sensor_list = data.get("RecordedSensorList", [])
    if not sensor_list:
        logger.warning("âš ï¸ RecordedSensorListê°€ ë¹„ì–´ìˆìŒ - ëª¨ë“  íŒŒì¼ ì˜¤í”„ì…‹ 0ìœ¼ë¡œ ì„¤ì •")
        return {file.upload_file_path: 0 for file in files}

    logger.info(f"ğŸ“Š ì„¼ì„œ ëª©ë¡: {len(sensor_list)}ê°œ ì„¼ì„œ ë°œê²¬")

    # âœ… í•µì‹¬ ìµœì í™”: í•´ì‹œë§µìœ¼ë¡œ íŒŒì¼ëª… ë§¤ì¹­ ì†ë„ í–¥ìƒ
    file_name_to_path = {os.path.basename(file.upload_file_path): file.upload_file_path
                         for file in files}
    logger.debug(f"ğŸ—‚ï¸ ì²˜ë¦¬í•  íŒŒì¼ëª…: {list(file_name_to_path.keys())}")

    file_start_times = {}
    failed_sensors = []

    for i, sensor in enumerate(sensor_list):
        sensor_name = sensor.get("Name", f"Sensor_{i}")
        sensor_files = sensor.get("Files", [])
        start_str = sensor.get("ActualStartTime")

        if not start_str:
            logger.warning(f"âš ï¸ ì„¼ì„œ '{sensor_name}': ActualStartTimeì´ ì—†ìŒ")
            continue

        if not sensor_files:
            logger.warning(f"âš ï¸ ì„¼ì„œ '{sensor_name}': íŒŒì¼ ëª©ë¡ì´ ì—†ìŒ")
            continue

        try:
            start_time_parsed = parse_time(start_str)
            matched_files = []

            # í•´ì‹œë§µ ë£©ì—…ìœ¼ë¡œ ë¹ ë¥¸ ë§¤ì¹­
            for file_name in sensor_files:
                if file_name in file_name_to_path:
                    file_start_times[file_name] = start_time_parsed
                    matched_files.append(file_name)

            if matched_files:
                logger.info(f"âœ… ì„¼ì„œ '{sensor_name}': {len(matched_files)}ê°œ íŒŒì¼ ë§¤ì¹­ (ì‹œì‘ì‹œê°„: {start_str})")
                logger.debug(f"   ğŸ“ ë§¤ì¹­ëœ íŒŒì¼: {matched_files}")
            else:
                logger.warning(f"âš ï¸ ì„¼ì„œ '{sensor_name}': ë§¤ì¹­ë˜ëŠ” íŒŒì¼ ì—†ìŒ")
                logger.debug(f"   ğŸ“ ì„¼ì„œ íŒŒì¼ ëª©ë¡: {sensor_files}")

        except Exception as e:
            failed_sensors.append(sensor_name)
            logger.error(f"âŒ ì„¼ì„œ '{sensor_name}' ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
            continue

    if failed_sensors:
        logger.warning(f"âš ï¸ ì‹¤íŒ¨í•œ ì„¼ì„œë“¤: {failed_sensors}")

    if not file_start_times:
        logger.warning("âš ï¸ ëª¨ë“  íŒŒì¼ì˜ ì‹œì‘ ì‹œê°„ì„ ì°¾ì„ ìˆ˜ ì—†ìŒ - ì˜¤í”„ì…‹ 0ìœ¼ë¡œ ì„¤ì •")
        return {file.upload_file_path: 0 for file in files}

    # ê¸°ì¤€ ì‹œê°„ ê³„ì‚°
    base_time = max(file_start_times.values())
    logger.info(f"â° ê¸°ì¤€ ì‹œê°„ ì„¤ì •: {base_time}")

    file_relative_offsets = {}
    matched_count = 0
    unmatched_files = []

    for file in files:
        file_path = file.upload_file_path
        file_name = os.path.basename(file_path)

        if file_name in file_start_times:
            actual_time = file_start_times[file_name]
            relative_us = int((actual_time - base_time).total_seconds() * 1_000_000)
            file_relative_offsets[file_path] = relative_us
            matched_count += 1
            logger.debug(f"ğŸ“„ {file_name}: ì˜¤í”„ì…‹ {relative_us:,}Î¼s")
        else:
            relative_us = 0
            file_relative_offsets[file_path] = relative_us
            unmatched_files.append(file_name)

    logger.info(f"âœ… ì˜¤í”„ì…‹ ê³„ì‚° ì™„ë£Œ: {matched_count}/{len(files)}ê°œ íŒŒì¼ ë§¤ì¹­")
    if unmatched_files:
        logger.warning(f"âš ï¸ ì‹œê°„ ì •ë³´ ì—†ëŠ” íŒŒì¼ë“¤ (ì˜¤í”„ì…‹ 0): {unmatched_files}")

    return file_relative_offsets


def run_visualization(server_url, user_id, project_id, blueprints, credentials):
    """ë©”ì¸ ì‹œê°í™” ì‹¤í–‰ í•¨ìˆ˜ - ì˜ˆì™¸ì²˜ë¦¬ ê°•í™”"""
    logger.info(f"ğŸš€ ì‹œê°í™” í”„ë¡œì„¸ìŠ¤ ì‹œì‘")
    logger.info(f"ğŸ‘¤ User ID: {user_id}")
    logger.info(f"ğŸ“ Project ID: {project_id}")
    logger.info(f"ğŸŒ Server URL: {server_url}")

    start_time = time.time()

    try:
        # ì…ë ¥ ë°ì´í„° ë””ì½”ë”©
        logger.info("ğŸ“‹ ì…ë ¥ ë°ì´í„° íŒŒì‹± ì¤‘...")
        blueprints_json = base64.b64decode(blueprints).decode("utf-8")
        credential_json = base64.b64decode(credentials).decode("utf-8")

        files = [FileMetadata(**f) for f in json.loads(blueprints_json)]
        credential = Credential(**json.loads(credential_json))

        logger.info(f"ğŸ“Š ì²˜ë¦¬í•  íŒŒì¼: {len(files)}ê°œ")
        for i, file in enumerate(files):
            logger.info(
                f"  {i + 1}. {file.entity_name} ({file.parser_name}): {os.path.basename(file.upload_file_path)}")

    except Exception as e:
        logger.error(f"âŒ ì…ë ¥ ë°ì´í„° íŒŒì‹± ì‹¤íŒ¨: {e}")
        logger.error(f"ğŸ“Š Blueprints ê¸¸ì´: {len(blueprints) if blueprints else 'None'}")
        logger.error(f"ğŸ”‘ Credentials ê¸¸ì´: {len(credentials) if credentials else 'None'}")
        raise

    try:
        # ì €ì¥ ê²½ë¡œ ì¤€ë¹„
        save_path = f"/data/rrd/{user_id}/{project_id}"
        os.makedirs(save_path, exist_ok=True)
        logger.info(f"ğŸ’¾ ì €ì¥ ê²½ë¡œ: {save_path}")

        # í ë° í”„ë¡œì„¸ì„œ ì¤€ë¹„
        queue = multiprocessing.Queue(maxsize=500)
        logger.info("ğŸ“® í ìƒì„± ì™„ë£Œ (í¬ê¸°: 20)")

        mqtt_topic = f"{user_id}/{project_id}"
        logger.info(f"ğŸ“¡ MQTT í† í”½: {mqtt_topic}")

        processor = DataProcessor(
            server_url=server_url,
            user_id=user_id,
            project_id=project_id,
            save_path=save_path,
            files=files,
            mqtt=get_mqtt_client(mqtt_topic),
            segment_duration_us=60_000_000,
            queue=queue,
        )
        processor.start()
        logger.info("âš™ï¸ ë°ì´í„° í”„ë¡œì„¸ì„œ ì‹œì‘ë¨")

    except Exception as e:
        logger.error(f"âŒ í”„ë¡œì„¸ì„œ ì´ˆê¸°í™” ì‹¤íŒ¨: {e}")
        logger.error(traceback.format_exc())
        raise

    try:
        # ë°°ë¦¬ì–´ ë° ìƒëŒ€ ì˜¤í”„ì…‹ ì¤€ë¹„
        barrier = SegmentBarrier()
        logger.info("ğŸš§ ì„¸ê·¸ë¨¼íŠ¸ ë°°ë¦¬ì–´ ìƒì„± ì™„ë£Œ")

        project_info_path = get_project_info_key(files[0].upload_file_path)
        logger.info(f"ğŸ“‹ í”„ë¡œì íŠ¸ ì •ë³´ ê²½ë¡œ: {project_info_path}")

        # âœ… ìµœì í™”ëœ ì˜¤í”„ì…‹ ê³„ì‚°
        file_relative_offsets = prepare_relative_offsets(project_info_path, files, credential)

    except Exception as e:
        logger.error(f"âŒ ì˜¤í”„ì…‹ ì¤€ë¹„ ì‹¤íŒ¨: {e}")
        logger.error(traceback.format_exc())
        logger.warning("ğŸ”„ ëª¨ë“  íŒŒì¼ ì˜¤í”„ì…‹ì„ 0ìœ¼ë¡œ ì„¤ì •í•˜ì—¬ ê³„ì† ì§„í–‰")
        file_relative_offsets = {file.upload_file_path: 0 for file in files}

    # íŒŒì¼ ì²˜ë¦¬ ìŠ¤ë ˆë“œ ì‹œì‘
    threads = []
    failed_threads = []

    logger.info(f"ğŸ”„ íŒŒì¼ ì²˜ë¦¬ ìŠ¤ë ˆë“œ ìƒì„± ì‹œì‘ ({len(files)}ê°œ)")

    for i, file in enumerate(files):
        try:
            relative_us = file_relative_offsets.get(file.upload_file_path, 0)

            dbc_path = f"/data/dbc/{user_id}/{file.dbc_file_name}" if file.dbc_file_name else None
            if dbc_path and not os.path.exists(dbc_path):
                logger.warning(f"âš ï¸ DBC íŒŒì¼ ì—†ìŒ: {dbc_path}")

            thread = FileProcessorThread(
                server_url=server_url,
                file_path=file.upload_file_path,
                parser_name=file.parser_name,
                entity_name=file.entity_name,
                dbc_file_path=dbc_path,
                selected_signals=file.selected_signals or [],
                user_project_key=f"{user_id}_{project_id}",
                queue=queue,
                barrier=barrier,
                segment_duration_us=60_000_000,
                relative_us=relative_us,
                credential=credential
            )

            thread.start()
            threads.append(thread)

            logger.info(f"âœ… ìŠ¤ë ˆë“œ {i + 1}/{len(files)} ì‹œì‘: {file.entity_name} (ì˜¤í”„ì…‹: {relative_us:,}Î¼s)")

        except Exception as e:
            failed_threads.append(f"{file.entity_name}: {e}")
            logger.error(f"âŒ ìŠ¤ë ˆë“œ ìƒì„± ì‹¤íŒ¨ ({i + 1}/{len(files)}): {file.entity_name} - {e}")
            continue

    if failed_threads:
        logger.error(f"âŒ ì‹¤íŒ¨í•œ ìŠ¤ë ˆë“œë“¤: {failed_threads}")

    if not threads:
        logger.error("âŒ ì‹œì‘ëœ ìŠ¤ë ˆë“œê°€ ì—†ìŒ - í”„ë¡œì„¸ìŠ¤ ì¢…ë£Œ")
        if processor.is_alive():
            processor.terminate()
        raise RuntimeError("No threads started successfully")

    logger.info(f"ğŸ¯ ì„±ê³µì ìœ¼ë¡œ ì‹œì‘ëœ ìŠ¤ë ˆë“œ: {len(threads)}/{len(files)}ê°œ")

    # í”„ë¡œì„¸ì„œ ì™„ë£Œ ëŒ€ê¸°
    try:
        logger.info("â³ ë°ì´í„° í”„ë¡œì„¸ì„œ ì™„ë£Œ ëŒ€ê¸° ì¤‘...")
        processor_start = time.time()
        processor.join()  # wait for rerun logging to finish
        processor_time = time.time() - processor_start

        logger.info(f"âœ… ë°ì´í„° í”„ë¡œì„¸ì„œ ì™„ë£Œ: {processor_time:.2f}ì´ˆ")

    except Exception as e:
        logger.error(f"âŒ í”„ë¡œì„¸ì„œ ëŒ€ê¸° ì¤‘ ì˜¤ë¥˜: {e}")
        if processor.is_alive():
            logger.warning("âš ï¸ í”„ë¡œì„¸ì„œ ê°•ì œ ì¢…ë£Œ")
            processor.terminate()
        raise

    # ì „ì²´ ì²˜ë¦¬ ì‹œê°„ ì¶œë ¥
    total_time = time.time() - start_time
    logger.info(f"ğŸ‰ ì‹œê°í™” í”„ë¡œì„¸ìŠ¤ ì™„ë£Œ!")
    logger.info(f"â±ï¸ ì´ ì²˜ë¦¬ ì‹œê°„: {total_time:.2f}ì´ˆ")
    logger.info(f"ğŸ“Š ì²˜ë¦¬ í†µê³„: {len(threads)}ê°œ íŒŒì¼, í‰ê·  {total_time / len(files):.2f}ì´ˆ/íŒŒì¼")


if __name__ == "__main__":
    parser = argparse.ArgumentParser(description="CODA ì‹œê°í™” í”„ë¡œì„¸ìŠ¤")
    parser.add_argument("--user-id", required=True, help="ì‚¬ìš©ì ID")
    parser.add_argument("--project-id", required=True, help="í”„ë¡œì íŠ¸ ID")
    parser.add_argument("--blueprints", required=True, help="ë¸”ë£¨í”„ë¦°íŠ¸ JSON (Base64)")
    parser.add_argument("--credential", required=True, help="í¬ë¦¬ë´ì…œ JSON (Base64)")
    parser.add_argument("--server-url", required=True, help="ì„œë²„ URL")
    parser.add_argument("--log-level", default="INFO",
                        choices=["DEBUG", "INFO", "WARNING", "ERROR"],
                        help="ë¡œê·¸ ë ˆë²¨ ì„¤ì •")

    args = parser.parse_args()

    # ë¡œê·¸ ë ˆë²¨ ì„¤ì •
    logging.getLogger().setLevel(getattr(logging, args.log_level))
    logger.info(f"ğŸ”§ ë¡œê·¸ ë ˆë²¨ ì„¤ì •: {args.log_level}")

    try:
        run_visualization(
            server_url=args.server_url,
            user_id=args.user_id,
            project_id=args.project_id,
            blueprints=args.blueprints,
            credentials=args.credential
        )
        logger.info("âœ… í”„ë¡œê·¸ë¨ ì •ìƒ ì¢…ë£Œ")

    except KeyboardInterrupt:
        logger.warning("âš ï¸ ì‚¬ìš©ìì— ì˜í•œ í”„ë¡œê·¸ë¨ ì¤‘ë‹¨ (Ctrl+C)")

    except Exception as e:
        logger.error(f"ğŸ’¥ í”„ë¡œê·¸ë¨ ì‹¤í–‰ ì¤‘ ì¹˜ëª…ì  ì˜¤ë¥˜: {e}")
        logger.error("ğŸ“‹ ìƒì„¸ ì˜¤ë¥˜ ì •ë³´:")
        logger.error(traceback.format_exc())
        exit(1)